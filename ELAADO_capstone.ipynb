{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85951,"databundleVersionId":9731193,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Video presentation is at \nhttps://drive.google.com/file/d/1nsvGHqaYam5apdTR2A27m1NrzxvT8aTy/view?usp=sharing\n\n***\n\n\n# TELECOM CUSTOMER CHURN PREDICTION\nTeam ELAADO is Aabiya Monsoor, Elena Boiko, and Don Krapohl\n\n## Video presentation\nhttps://drive.google.com/file/d/1nsvGHqaYam5apdTR2A27m1NrzxvT8aTy/view?usp=sharing\n\n## Problem Definition\nThe primary task of this project is to predict customer churn for a telecommunications company. Customer churn refers to the phenomenon where customers discontinue their subscription to a service or switch to another provider. The dataset used for this project includes demographic, service-related, and billing information for customers. Features such as contract type, monthly charges, payment methods, and tenure are crucial in identifying customers who are at risk of leaving the service.\n\nThe telecommunications industry is highly competitive, with an annual churn rate of 15-25%. Reducing churn is critical because retaining existing customers is more cost-effective than acquiring new ones. By accurately predicting churn, businesses can implement targeted retention strategies, reducing attrition and improving profitability.\n\n## Project Goal\nThe goal of this project is to develop a machine learning model that can accurately predict whether a customer will churn. \n\n**The key objectives are:**\n\n1) Achieve a high level of classification performance, targeting a **ROC-AUC** score of at least 80%.\n\n2) Provide actionable insights into factors contributing to churn to assist in retention strategies.\n\n## Approach\nTo meet the project objectives, the following steps were undertaken:\n\n### 1. Data Preprocessing:\n\nAddressed missing or invalid values in the total_charges column by imputing them with the mean and converting the column to numeric.\nEncoded categorical variables using LabelEncoder and one-hot encoding as appropriate.\nScaled numerical features using StandardScaler and MinMaxScaler to ensure compatibility with machine learning algorithms.\nSplit the dataset into training and test sets with stratification to preserve the class distribution.\n\n### 2. Feature Engineering:\n\nPerformed feature importance analysis using Random Forest to identify key predictors of churn.\nExplored dimensionality reduction using Principal Component Analysis (PCA) to evaluate its impact on model performance.\n\n### 3. Model Development:\n\n**Implemented and tuned various machine learning models:**\n\nLogistic Regression\n\nSupport Vector Machines (SVM)\n\nDecision Trees\n\nRandom Forest\n\nK-Nearest Neighbors (KNN)\n\nEnsemble methods (Voting Classifier, Bagging, XGBoost)\n\nApplied SMOTE to address class imbalance and improve the minority class representation.\nConducted hyperparameter tuning using RandomizedSearchCV to optimize model performance.\n\n### 4. Model Evaluation:\n\nEvaluated models using metrics such as accuracy, F1-score, precision, recall, and ROC-AUC.\nSelected the best-performing model based on test ROC-AUC and interpretability.\nCompared advanced models with baseline Logistic Regression to determine the most effective approach.\n\n## Motivation\nCustomer churn poses a significant challenge for telecom companies, directly impacting revenue and market position. By developing a predictive churn model, this project aims to:\n\nHelp businesses identify customers at high risk of leaving.\nEnable targeted retention strategies, focusing resources on high-value customers.\nProvide actionable insights into the drivers of churn, facilitating data-driven decision-making.\nReducing churn not only preserves a companyâ€™s customer base but also improves profitability by lowering acquisition costs and increasing lifetime value. A robust churn prediction model equips telecom companies to thrive in a competitive market and maintain customer loyalty effectively.","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"# basic dataframe and operations\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# manipulation and preprocessing\nfrom sklearn.preprocessing import Normalizer, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nimport xgboost as xgb\n\n# measuring results\nfrom sklearn.metrics import accuracy_score, f1_score, precision_recall_curve,confusion_matrix, mean_absolute_error, roc_auc_score, precision_score, recall_score, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# warning suppression\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:06.928004Z","iopub.execute_input":"2024-12-02T16:04:06.928384Z","iopub.status.idle":"2024-12-02T16:04:11.062782Z","shell.execute_reply.started":"2024-12-02T16:04:06.928351Z","shell.execute_reply":"2024-12-02T16:04:11.061671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\nIn this section, we performed EDA to understand the dataset, check for missing values, visualize data distributions, and analyze correlations between features.\n\n### 1. Loading and Inspecting the Dataset","metadata":{}},{"cell_type":"code","source":"# Import the csv training dataset to a pandas dataframe\ndata_raw_input = pd.read_csv('/kaggle/input/customer-churn-prediction-fall-2024/train.csv')\n\n# Show the shape of the dataset\ndata_raw_input.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:17.705989Z","iopub.execute_input":"2024-12-02T16:04:17.706497Z","iopub.status.idle":"2024-12-02T16:04:17.759198Z","shell.execute_reply.started":"2024-12-02T16:04:17.706460Z","shell.execute_reply":"2024-12-02T16:04:17.758225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset contains 5634 rows (observations) and 21 columns (features), including the target variable (Churn).","metadata":{}},{"cell_type":"markdown","source":"### 2. Displaying a Sample of the Training Data\n\nTo better understand the dataset, we displayed the first 20 rows, including all 21 columns, to examine the data structure and contents.","metadata":{}},{"cell_type":"code","source":"# Display a few rows from the training data\ndata_raw_input.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:21.256311Z","iopub.execute_input":"2024-12-02T16:04:21.256695Z","iopub.status.idle":"2024-12-02T16:04:21.301781Z","shell.execute_reply.started":"2024-12-02T16:04:21.256661Z","shell.execute_reply":"2024-12-02T16:04:21.300725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataset Overview:**\n\nThe dataset consists of 21 columns, including features such as gender, senior_citizen, tenure, and monthly_charges, as well as the target variable label (indicating customer churn).\n\n**Row Example:** Each row represents a customer, with attributes such as demographics, subscription details, and payment information.\n\n**Target Variable (label):**\n0: Customer did not churn.\n1: Customer churned.","metadata":{}},{"cell_type":"markdown","source":"### 3. Checking for Missing Values, Duplicates, and Data Issues\n\nIn this step, we checked for null values, missing data (NaNs), and duplicate rows to ensure data integrity. Additionally, we identified columns with invalid values such as strings with spaces in numeric fields.","metadata":{}},{"cell_type":"code","source":"# Get the count of nulls per column\n# Turns out we don't have any\nprint(\"Nulls:\")\nprint(data_raw_input.isnull().sum().sum())\nprint(\"Na count:\")\nprint(data_raw_input.isna().sum().sum())\nprint(\"Duplicate rows:\")\nprint(data_raw_input.duplicated(keep='first').sum())\n\n# In attempting to change total_charges from string to numeric I received an error that there were some\n#   values in there that were just ' ' -- a non-empty string that contains just a space. Now I'll formally detect it.\nprint(data_raw_input.columns[data_raw_input.isin([' ']).any()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:26.612647Z","iopub.execute_input":"2024-12-02T16:04:26.613037Z","iopub.status.idle":"2024-12-02T16:04:26.655185Z","shell.execute_reply.started":"2024-12-02T16:04:26.613006Z","shell.execute_reply":"2024-12-02T16:04:26.654026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Results:**\n\n1) Null Values: No null values were found in the dataset (Nulls: 0).\n\n2) NaNs: Similarly, no missing values (NaNs) were detected (Na count: 0).\n\n3) Duplicates: The dataset has no duplicate rows (Duplicate rows: 0).\n\n4) Invalid Values: \nThe column **total_charges** contains invalid entries: strings with spaces (' ') instead of numeric values.","metadata":{}},{"cell_type":"markdown","source":"## 2. Data Preprocessing\nIn this section, we addressed data preprocessing tasks to prepare the dataset for machine learning models and ensured reusability of the preprocessing pipeline for future predictions.\n\n### 2.1 Dealing with total_charges\nThe column **total_charges** contained invalid entries (empty strings with spaces ' ') and needed to be converted to a numeric data type for analysis and modeling. So, the column needs to be changed to float. We'll first run an imputer over them to replace that, then we'll change to numeric data type.\n\n#### Summary of Results:\n**1. Preprocessing of total_charges:**\n\nSuccessfully handled invalid values, imputed missing entries, and converted the column to numeric.\nEnsured that preprocessing could be applied to both training and test data for consistency.\n\n**2. Reusable Pipeline:**\n\nDefined reusable, modular functions for hyperparameter tuning, feature importance visualization, and test set predictions.\n\nSimplified the workflow for future model evaluation and submission.","metadata":{}},{"cell_type":"code","source":"# Class: Multiprep\n# Purpose: To preprocess all features. \nclass MultiPrep:\n    feature_encoders={}\n    def destroy_encoders(self):\n        self.feature_encoders = {}                   # overwrite the old encoders with an empty collection\n        \n    def fit_text_encoders(self, df_features):\n        non_numeric_columns = df_features.select_dtypes(exclude='number').columns   # get a list of non-numeric cols\n        for col in non_numeric_columns:         # loop through those columns to fit an encoder\n            encoder = LabelEncoder()            # make a new label encoder for this column\n            encoder.fit(df_features[col])       # fit this encoder\n            self.feature_encoders[col] = encoder     # add it to the encoders collection for later use\n\n    # fit a normalizer for all numerics\n    def fit_scaler(self, df_features):\n        # note I'm not caching the \"this model has not been fit\" error, like I would for a real app\n        numeric_columns = df_features.select_dtypes(include='number').columns   # get a list of numeric cols\n        scaler = StandardScaler()\n        scaler.fit(df_features[numeric_columns])    # apply the transform\n        self.feature_encoders['numerics'] = scaler  # preserve the numeric scaler \n     \n    # encode all labels\n    def transform_encode_all(self, df_features):\n        # note I'm not caching the \"this model has not been fit\" error, like I would for a real app\n        non_numeric_columns = df_features.select_dtypes(exclude='number')   # get a list of non-numeric cols\n        for col in non_numeric_columns:         # loop through them\n            df_features[col] = self.feature_encoders[col].transform(df_features[col])    # apply the transform\n        return df_features     \n    \n    # normalize all numerics\n    def transform_scale_all(self, df_features):\n        # note I'm not caching the \"this model has not been fit\" error, like I would for a real app\n        numeric_columns = df_features.select_dtypes(include='number').columns   # get a list of numeric cols\n        df_features[numeric_columns] = self.feature_encoders['numerics'].transform(df_features[numeric_columns])    # apply the transform\n        return df_features\n            \n    def decode_all(self, df_features):\n        non_numeric_columns = df_features.select_dtypes(exclude='number')   # get the non-numeric cols\n        for col in non_numeric_columns:         # loop through them\n            df_features[col] = self.feature_encoders[col].inverse_transform(df_features[col])    # decode the data\n    \n    \n# split the x and y data. Doing it outside the transformation as we don't want to transform the validation\n#   set yet\ndef split_x_y(data_raw):\n    # remove the result column from the input parameters\n    # also remove the ID column. It carries no signal.\n    X_no_label = data_raw.drop('label', axis=1).drop('id', axis=1)\n\n    # Assign class labels for the input data\n    y_labels = data_raw['label']      # assign the labels we'll encode in the next block\n    return X_no_label, y_labels\n\n\n# method: transform_features\n# purpose: to clean and encode the features of a dataset\n# parameters: X_without_label - the raw features without label or id columns\n#       transform_only - True or False. If it's the training set we fit and transform otherwise transform only\n# returns: X_scaled - cleaned features that are label encoded and scaled\n# steps:\n#  Drop the label and id from the x\n#  Put the label into y\n#  replace spaces in total_charges with Nan\n#  Recast total_charges to float\n#  impute the missing values in total_charges\n#  encode the string data\n#  scale the numeric data\n#  return the x data\ndef transform_features(X_without_label, transform_only):\n   \n    # --------------- total_charges processing --------------------\n    # change spaces in total_charges to Nan then recast\n    X_without_label['total_charges'] = X_without_label['total_charges'].replace(' ', np.nan)\n    X_without_label['total_charges'] = pd.to_numeric(X_without_label['total_charges']) # recast as floating point\n    print(X_without_label['total_charges'])\n    # Now we're missing values so let's impute them\n    imputer = SimpleImputer(strategy='mean')\n    X_without_label['total_charges'] = imputer.fit_transform(X_without_label['total_charges'].values.reshape(-1,1))\n    \n    # --------------- end total_charges processing\n      \n     \n    # Encode the string columns and scale the numerics\n    if not transform_only:                           # only fit the model if it's the x training set, not validate or predict\n        multiprep.fit_text_encoders(X_without_label)                    # fit the encoders\n        multiprep.fit_scaler(X_without_label)                      # fit the normalizer\n      \n    X_without_label = multiprep.transform_scale_all(X_without_label) # normalize all of the numeric columns \n    X_without_label = multiprep.transform_encode_all(X_without_label) # encode all of the text columns \n    \n        \n    return X_without_label\n\n# method: tune_model\n# purpose: to use cross-validation to find the best HPs, fit a model, and do basic scoring on it\n# parameters:\n#   model - a defined but not already-fit model to search for the best hyperparameters\n#   X_from_train - the features from the training set. Calling it this because I don't know which processed version I'll use.\n#   y_from_train - the classes for each sample in the training set\n# returns:\n#   model - the trained model using the \"best\" found hyperparameters\ndef tune_model(model, param_grid, X_from_train, y_from_train, scoring='roc_auc'):\n    rscv = RandomizedSearchCV(\n        estimator=model,\n        param_distributions=param_grid,\n        scoring=scoring,\n        cv=5,\n        random_state=17,\n        refit=True)\n    rscv = rscv.fit(X_from_train, y_from_train)  # xtrainsig was 0.81, train balanced .834, trainwolabel 0.767\n    print(rscv.best_score_)\n    print(rscv.best_params_)\n\n    model.fit(X_from_train, y_from_train)\n    return model\n\n# method: plot_importances\n# purpose: shows class labels and graphs their contribution to the training variance in descending order (scale 0-1.0)\n# parameters:\n#   X_from_train - the features from the training set\n#   features_labels - the column labels for those features in plaintext\n#   importances - the relative importance of each feature (scaled 0-1.0)\n# returns:\n#   none. Print only.\ndef plot_importances(X_from_train, feature_labels, importances):\n    # graphing of most important features from chapter 6 class notes\n    indices = np.argsort(importances)[::-1]\n    for f in range(X_from_train.shape[1]):\n        print(\"%2d) %-*s %f\" % (f + 1, 30,feature_labels[indices[f]],importances[indices[f]]))\n\n    plt.title('Feature importance')\n    plt.bar(range(X_from_train.shape[1]),\n        importances[indices],\n        align='center')\n    plt.xticks(range(X_from_train.shape[1]),\n        feature_labels[indices], rotation=90)\n    plt.xlim([-1, X_from_train.shape[1]])\n    plt.tight_layout()\n    plt.show()\ndef write_predictions(model, test_input_path='/kaggle/input/customer-churn-prediction-fall-2024/test.csv', \n                      predict_out_path='submission.csv'):\n    # Do predictions on the submission test set and save the output as csv\n    data_test_input = pd.read_csv(test_input_path) # get the test inputs\n\n    df_output = pd.DataFrame()\n    # remove the ID column and the save it in output_ids\n    df_output['id'] = data_test_input['id']                 # set the IDs we'll output but don't predict on them\n    data_test_input = data_test_input.drop('id', axis=1)    # drop the id columns\n\n    # Need to encode the test data\n    # This uses the tranformers we trained earlier as feature_encoder\n    X_test_for_out = transform_features(data_test_input, transform_only=True)\n\n    # Get the probabilities of class 1 (will churn)\n    prob_for_samples = model.predict_proba(X_test_for_out)  # get the predictions of both classes\n    df_output['label'] = prob_for_samples[:,1]              # write the label column as the predictions of class 1\n\n    df_output.to_csv(predict_out_path, index=False) # write the csv\n    print(\"Predictions written\")                            # print a message\n    \nlabel_encoders={}\nmultiprep = MultiPrep()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:30.221003Z","iopub.execute_input":"2024-12-02T16:04:30.221373Z","iopub.status.idle":"2024-12-02T16:04:30.243099Z","shell.execute_reply.started":"2024-12-02T16:04:30.221343Z","shell.execute_reply":"2024-12-02T16:04:30.242034Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Identifying Unique Values in Non-Numeric Columns\nIn this step, we examined all non-numeric columns in the dataset to identify categorical features and their unique values. This helps determine the appropriate preprocessing steps, such as encoding categorical variables and re-typing numeric columns stored as strings.","metadata":{}},{"cell_type":"code","source":"# For the label column in the training set\n# Show the unique values of the training labels\ncol_list = data_raw_input.columns.to_list()\n\nfor col in data_raw_input.columns:\n    if not pd.api.types.is_numeric_dtype(data_raw_input[col]):\n        print(\"{}: {}\".format(col, data_raw_input[col].unique()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:40.726978Z","iopub.execute_input":"2024-12-02T16:04:40.727367Z","iopub.status.idle":"2024-12-02T16:04:40.742157Z","shell.execute_reply.started":"2024-12-02T16:04:40.727334Z","shell.execute_reply":"2024-12-02T16:04:40.741170Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Observations:\n**Categorical Features:**\nColumns like **gender, partner, dependents**, and **payment_method** are categorical and need encoding (e.g., one-hot or label encoding).\n\n**Non-Numeric Column for Numeric Data:**\nThe column **total_charges** contains numeric data stored as strings. Before any encoding, this column must:\nBe converted from string to numeric using pd.to_numeric() after handling invalid entries (' ').\n\n**Ordinal Features:**\nNo clear ordinal features are identified in the dataset. All categorical features appear nominal (no inherent order).\n\n**Special Cases:**\nFeatures like **multiple_lines, internet_service**, and **streaming_tv** include specific categories like 'No phone service' and 'No internet service', which may require careful encoding to preserve meaningful distinctions.","metadata":{}},{"cell_type":"markdown","source":"## 3. Visualisation\n### 3.1 Test for imbalance\n\nWe can see that the classes are imbalanced. We'll generate more samples after the train/test split for the minority class.","metadata":{}},{"cell_type":"code","source":"# Get counts for each class to see how imbalanced the classes are if at all\nplt.figure(figsize=(8, 6))\nplt.title('Class Imbalance Check', fontsize=16)\n\n# Calculate normalized percentages and prepare the DataFrame for visualization\nlabels = data_raw_input['label'].value_counts(normalize=True).rename_axis('label').reset_index(name='Percentage')\n\n# Create the bar plot\nbar_plot = sns.barplot(x='label', y='Percentage', data=labels, palette='pastel')\n\n# Annotate the bars with percentage values\nfor p in bar_plot.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    bar_plot.annotate(f'{height:.0%}', (x + width/2, y + height*1.02), ha='center', fontweight='bold')\n\n# Add labels\nplt.xlabel('Churn (0 = No, 1 = Yes)', fontsize=12)\nplt.ylabel('Percentage', fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n# Print raw counts for reference\ncounts = data_raw_input['label'].value_counts()\nprint(counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:47.021300Z","iopub.execute_input":"2024-12-02T16:04:47.021763Z","iopub.status.idle":"2024-12-02T16:04:47.421072Z","shell.execute_reply.started":"2024-12-02T16:04:47.021726Z","shell.execute_reply":"2024-12-02T16:04:47.419971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The **dataset is imbalanced**, with a significant skew towards Class 0 (No Churn). \n\n**This imbalance can:** \nLead to bias in machine learning models, where the model may prioritize predicting the majority class (0) while underperforming on the minority class (1).\nNegatively impact performance metrics such as recall and F1-score for Class 1.","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Correlation Heatmap for numerical features","metadata":{}},{"cell_type":"code","source":"# Plot correlation heatmap\n\n# Filter numerical columns only (excluding 'id')\nnumerical_columns = data_raw_input.select_dtypes(include='number').drop(columns=['id']).columns\n\n# Calculate the correlation matrix for numerical features\nnumerical_correlation = data_raw_input[numerical_columns].corr()\n\n# Plot the heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(numerical_correlation, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Heatmap for Numerical Features')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:04:54.048439Z","iopub.execute_input":"2024-12-02T16:04:54.048820Z","iopub.status.idle":"2024-12-02T16:04:54.341138Z","shell.execute_reply.started":"2024-12-02T16:04:54.048784Z","shell.execute_reply":"2024-12-02T16:04:54.339986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"None of the correlations are very strong (>0.5), indicating that these features alone may not be strong predictors of the label. Other features or engineered variables might be needed to improve predictions.","metadata":{}},{"cell_type":"markdown","source":"### 3.3 Churn vs. Numeric Features\nThese plots visualize the distribution of key numeric features (tenure, monthly_charges, total_charges) against churn to understand their relationship with customer retention, as these features represent customer engagement, pricing, and overall spending, which are critical factors influencing churn behavior.","metadata":{}},{"cell_type":"code","source":"# Suppress warnings from pandas and seaborn\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)  # Ignore FutureWarnings\npd.options.mode.use_inf_as_na = True  # Treat inf values as NaN\n\n# Define numeric features to plot\nnumeric_features = ['tenure', 'monthly_charges', 'total_charges']\n\n# Plot distributions for each numeric feature against churn\nfor feature in numeric_features:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(\n        data=data_raw_input,\n        x=feature,\n        hue='label',\n        kde=True,\n        palette='viridis',\n        bins=30,\n        element=\"step\"  # Cleaner plot style\n    )\n    plt.title(f'Distribution of {feature} by Churn')\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:05:06.234017Z","iopub.execute_input":"2024-12-02T16:05:06.234418Z","iopub.status.idle":"2024-12-02T16:05:42.076235Z","shell.execute_reply.started":"2024-12-02T16:05:06.234379Z","shell.execute_reply":"2024-12-02T16:05:42.075129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Conclusion from Tenure, Monthly Charges, and Total Charges Analysis\n**1. Tenure Insights:**\n\nHigh churn rate for new customers: Customers with low tenure are much more likely to churn, suggesting that newer customers are less engaged or satisfied.\nLoyalty increases with tenure: Customers with high tenure are significantly less likely to churn, indicating that long-term customers tend to stay loyal.\n\n**2. Monthly Charges Insights:**\n\nHigher churn rate with higher charges: Customers paying higher monthly charges are more likely to churn, potentially due to dissatisfaction with pricing or perceived value.\n\n**3. Total Charges Insights:**\n\nLoyalty linked to cumulative spending: Customers with higher total charges (long-term, high-value customers) appear more loyal, reflecting their prolonged engagement and satisfaction.\nChurn risk for new customers: Lower total charges are associated with higher churn, indicating that newer or less-engaged customers are at greater risk.","metadata":{}},{"cell_type":"markdown","source":"### 3.4 Categorical Features and Churn","metadata":{}},{"cell_type":"code","source":"# Bar plot for categorical features\ncategorical_features = ['contract', 'payment_method', 'online_security', 'gender', \n                        'paperless_billing', 'tech_support', 'multiple_lines']\n\nfor feature in categorical_features:\n    plt.figure(figsize=(10, 6))\n    churn_percentage = data_raw_input.groupby(feature)['label'].mean().reset_index()\n    sns.barplot(data=churn_percentage, x=feature, y='label', palette='pastel')\n    plt.title(f'Churn Rate by {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Churn Rate')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:06:06.909404Z","iopub.execute_input":"2024-12-02T16:06:06.909826Z","iopub.status.idle":"2024-12-02T16:06:08.561771Z","shell.execute_reply.started":"2024-12-02T16:06:06.909792Z","shell.execute_reply":"2024-12-02T16:06:08.560631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.5 Visualizing Churn Distribution for Categorical Features\nWe used these plots to visualize the churn distribution across categorical features, enabling us to identify patterns or relationships between customer attributes and churn behavior for more informed feature selection and analysis.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Filter object-type columns, excluding the target variable ('label')\ncategorical_features = [col for col in data_raw_input.select_dtypes(include='object').columns if col != 'label']\n\n# Dynamically determine the number of rows and columns based on the number of categorical features\nnum_features = len(categorical_features)\nnum_rows = (num_features // 4) + (1 if num_features % 4 != 0 else 0)  # Calculate rows needed\n\n# Set up the figure and axes for subplots\nfig, axes = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))  # Adjust height dynamically\naxes = axes.flatten()  # Flatten to make indexing easier\n\n# Generate bar plots for each categorical feature\nfor i, col in enumerate(categorical_features):\n    churn_percentage = data_raw_input.groupby(col)['label'].mean().reset_index()\n    sns.barplot(data=churn_percentage, x=col, y='label', palette='pastel', ax=axes[i])\n    axes[i].set_title(f'{col} vs Churn')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Churn Rate')\n\n# Remove any unused subplot axes\nfor j in range(len(categorical_features), len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:06:15.772822Z","iopub.execute_input":"2024-12-02T16:06:15.773232Z","iopub.status.idle":"2024-12-02T16:07:13.864230Z","shell.execute_reply.started":"2024-12-02T16:06:15.773198Z","shell.execute_reply":"2024-12-02T16:07:13.863135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Train/Test Split and Preprocessing\n1) Split the dataset into 70% training data and 30% testing data using a stratified split to ensure consistent class distribution across subsets.\n   \n2) Applied transformations to the features, such as encoding categorical variables and scaling numerical ones, using a reusable pipeline (transform_features).\n\n### 4.1 Train/Test Split ","metadata":{}},{"cell_type":"code","source":"# Split the features and the class labels\nX_without_label, y_values = split_x_y(data_raw_input)   # Separate features (X) and target variable (y)\n\n# Perform a 70/30 train-test split\n# Stratification ensures class distribution remains consistent across train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_without_label, y_values,\n    test_size=0.3,\n    stratify=y_values, \n    random_state=17)\n\n# Apply preprocessing (e.g., encoding, scaling) to training and testing sets\n# The senior_citizen column is already encoded as 0/1; scaling will retain its interpretability\n\nX_train= transform_features(X_train, transform_only=False) # Fit and transform on training data\nX_test = transform_features(X_test, transform_only=True) # Fit and transform on testing data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:07:22.044535Z","iopub.execute_input":"2024-12-02T16:07:22.045358Z","iopub.status.idle":"2024-12-02T16:07:22.108482Z","shell.execute_reply.started":"2024-12-02T16:07:22.045320Z","shell.execute_reply":"2024-12-02T16:07:22.107284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2 Transforming senior_citizen Separately\n\nCheck how the senior_citizen column appears after preprocessing and transformation.\nsenior_citizen is already encoded as 0 (not a senior) or 1 (senior), so its transformation should preserve this representation.\n\nDisplay the first 25 rows of the training and testing sets after applying transformations to verify correctness.","metadata":{}},{"cell_type":"code","source":"# Display the first 25 rows of training and testing sets to examine transformations\nprint(X_train[:25]) # First 25 rows of the training set\nprint(X_test[:25]) # First 25 rows of the testing set","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:07:28.761587Z","iopub.execute_input":"2024-12-02T16:07:28.761992Z","iopub.status.idle":"2024-12-02T16:07:28.782294Z","shell.execute_reply.started":"2024-12-02T16:07:28.761958Z","shell.execute_reply":"2024-12-02T16:07:28.781086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Verification:** Ensures transformations are applied correctly without introducing unexpected issues.\n\n**Validation:** Confirms that preprocessing preserves feature integrity (e.g., senior_citizen still conveys the same meaning after scaling).","metadata":{}},{"cell_type":"markdown","source":"### 4.3 Balancing the Dataset: Generating Synthetic Samples for the Minority Class\nIn this step, we address the class imbalance observed in the training dataset. The minority class is significantly smaller (around one-third the size of the majority class). To ensure the model is not biased towards the majority class, we use **Synthetic Minority Oversampling Technique (SMOTE)** to generate synthetic samples for the minority class, bringing the two classes into balance.","metadata":{}},{"cell_type":"code","source":"# Generate synthetic samples for the minority class using SMOTE\nsmote = SMOTE(random_state=17)\nX_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)\n\nfrom collections import Counter\nprint(\"Original class distribution:\", Counter(y_train))\nprint(\"Resampled class distribution:\", Counter(y_train_sampled))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:07:37.493976Z","iopub.execute_input":"2024-12-02T16:07:37.494385Z","iopub.status.idle":"2024-12-02T16:07:37.571410Z","shell.execute_reply.started":"2024-12-02T16:07:37.494352Z","shell.execute_reply":"2024-12-02T16:07:37.570560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Description of Results:\n**Original Class Distribution:**\n\nThe training set had a class imbalance, with the minority class comprising roughly one-third of the total samples.\n\n**After SMOTE Sampling:**\n\nThe fit_resample method generated synthetic samples for the minority class until the classes were balanced.\nBoth the majority and minority classes now have the same number of samples.","metadata":{}},{"cell_type":"markdown","source":"### 4.4 Feature Importance Ranking\n\nThis step evaluates the **importance of features** in the dataset by calculating their relative contribution to the model's decision-making process. Understanding feature importance helps identify the most influential predictors and can guide feature selection or dimensionality reduction.","metadata":{}},{"cell_type":"code","source":"# Exclude 'id' from feature labels\nfeature_labels = data_raw_input.drop(columns=['id']).columns  # Remove 'id' column\n\n# Train the Random Forest Classifier\nrf_manual = RandomForestClassifier(max_features=40, n_estimators=200, criterion='gini', n_jobs=-1, random_state=17)\nrf_manual.fit(X_train, y_train)  # Fit the model\n\n# Get feature importances\nimportances = rf_manual.feature_importances_\n\n# Plot the feature importances\nplot_importances(X_train, feature_labels, importances)  # Graph the importance of each feature\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:07:42.151773Z","iopub.execute_input":"2024-12-02T16:07:42.152186Z","iopub.status.idle":"2024-12-02T16:07:43.902880Z","shell.execute_reply.started":"2024-12-02T16:07:42.152151Z","shell.execute_reply":"2024-12-02T16:07:43.901774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Top Features:**\n\npayment_method, monthly_charges, and streaming_movies are the most important predictors, significantly influencing the model's performance.\nThese features likely correlate strongly with customer churn.\n\n**Less Important Features:**\n\nFeatures such as tenure, multiple_lines, and tech_support have minimal contribution to the model's decision-making process.\n","metadata":{}},{"cell_type":"markdown","source":"## 5. Model Initialization, Training, and Evaluation \n**Model Evaluation:**\nPredictions are made on the test set (X_test), and the model's performance is evaluated using: \n\n**Accuracy Score:** Percentage of correct predictions.\n\n**F1-Score:** Balance between precision and recall, especially useful for imbalanced datasets.\n\n**ROC-AUC Score:** Evaluates the model's ability to distinguish between classes.\n### 5.1 Logistic Regression \n### 5.1.1 Baseline Model with Hyperparameter Tuning\nThe goal of this step is to train a Logistic Regression (LR) model on the entire feature set and evaluate its performance. The model is optimized by tuning key hyperparameters (solver and C) using RandomizedSearchCV. This serves as a baseline for comparison with future models and dimensionality reduction techniques.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n\n# Define the parameter grid for tuning\nparam_grid = {'solver': ['liblinear', 'newton-cg', 'saga', 'lbfgs'], \n              'C': [0.001, 0.01, 1.0, 3.0, 5.0, 7.0, 10.0, 12.0],\n             }\n\n# Tune the hyperparameters \"solver\" and \"C\" for the LR model using RandomizedSearchCV\nlr_baseline = tune_model(LogisticRegression(random_state=17, max_iter=1000), \n                         param_grid, \n                         X_train, \n                         y_train, \n                         scoring='roc_auc')\n\n# Compute predicted probabilities for Logistic Regression Baseline\ny_test_proba_lr_baseline = lr_baseline.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = lr_baseline.predict(X_test)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_lr_baseline)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_lr, tpr_lr, _ = roc_curve(y_test, y_test_proba_lr_baseline)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:07:54.427818Z","iopub.execute_input":"2024-12-02T16:07:54.428578Z","iopub.status.idle":"2024-12-02T16:07:57.580692Z","shell.execute_reply.started":"2024-12-02T16:07:54.428518Z","shell.execute_reply":"2024-12-02T16:07:57.576903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis of Logistic Regression (Baseline):\n\nThe accuracy of 80% is reasonable but not exceptional, given the imbalance in the dataset.\nThe F1-Score of ~60% indicates that the model struggles with precision and recall on the minority class.\nThe AUC-ROC score of 83.83% demonstrates good discriminatory power, meaning the model is reasonably effective at distinguishing between the positive and negative classes. While better than a random guess (AUC = 0.50), there is still room for improvement in separating the classes more confidently.\n","metadata":{}},{"cell_type":"markdown","source":"### 5.1.2 PCA and Logistic Regression: Dimensionality Reduction and Evaluation\nThis step evaluates the effect of Principal Component Analysis (PCA) as a dimensionality reduction technique on the performance of a Logistic Regression model. PCA reduces the feature space to 6 components, aiming to simplify the model while retaining most of the variance in the dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\nfrom sklearn.decomposition import PCA\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=6)                   # Reduce to 6 components based on prior analysis\nX_pca = pca.fit_transform(X_train)          # Fit and transform the training data\n\n# Tune Logistic Regression on the PCA-transformed data\nparam_grid = {'solver': ['liblinear', 'newton-cg', 'saga', 'lbfgs'], \n              'C': [0.001, 0.01, 1.0, 3.0, 5.0, 7.0, 10.0, 12.0]}\n\n# Tune the hyperparameters \"solver\" and \"C\" for the LR model using RandomizedSearchCV cross-validation\nlr_pca = tune_model(LogisticRegression(random_state=17, max_iter=1000), \n                    param_grid, \n                    X_pca, \n                    y_train, \n                    scoring='roc_auc')\n\n# Transform test data using the same PCA model\nX_test_pca = pca.transform(X_test)\n\n# Compute predicted probabilities for Logistic Regression on PCA-transformed data\ny_test_proba_lr_pca = lr_pca.predict_proba(X_test_pca)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = lr_pca.predict(X_test_pca)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_lr_pca)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_lr_pca, tpr_lr_pca, _ = roc_curve(y_test, y_test_proba_lr_pca)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:08:04.861671Z","iopub.execute_input":"2024-12-02T16:08:04.862052Z","iopub.status.idle":"2024-12-02T16:08:05.986722Z","shell.execute_reply.started":"2024-12-02T16:08:04.862020Z","shell.execute_reply":"2024-12-02T16:08:05.982965Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Summary of PCA Impact:\n**Dimensionality Reduction:**\nPCA reduced the feature set to 6 components, simplifying the model but causing information loss, which lowered test performance.\n\n**Baseline vs. PCA Performance:**\nBaseline Accuracy: 80.01% (without PCA).\nPCA Accuracy: 78.30%.\nRetaining the full feature set appears more effective for this dataset.\n\n**Impact on F1-Score and AUC:**\nPCA reduced both F1-Score (57.11%) and AUC (82.22%) compared to the baseline, indicating weaker performance in balancing class predictions.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.3 Feature Selection with Logistic Regression\nThis step explores whether feature selection improves model performance by using a threshold-based method (SelectFromModel) to remove less important features before training a Logistic Regression model.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output\n\n# Train Logistic Regression on the full feature set\nlr_sel = LogisticRegression(random_state=17, max_iter=1000).fit(X_train, y_train)\n\n# Select important features using SelectFromModel\nsfm = SelectFromModel(estimator=lr_sel, prefit=True, threshold=0.05)  # Select features based on importance\nX_train_selected = sfm.transform(X_train)\nX_test_selected = sfm.transform(X_test)\n\n# Tune hyperparameters on the selected feature set\nlr_sel = tune_model(\n    LogisticRegression(random_state=17, max_iter=1000),\n    param_grid, \n    X_train_selected, \n    y_train, \n    scoring='roc_auc'\n)\n\n# Compute predicted probabilities for Logistic Regression on the selected feature set\ny_test_proba_lr_sel = lr_sel.predict_proba(X_test_selected)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = lr_sel.predict(X_test_selected)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_lr_sel)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_lr_sel, tpr_lr_sel, _ = roc_curve(y_test, y_test_proba_lr_sel)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:08:14.952513Z","iopub.execute_input":"2024-12-02T16:08:14.952938Z","iopub.status.idle":"2024-12-02T16:08:16.895439Z","shell.execute_reply.started":"2024-12-02T16:08:14.952904Z","shell.execute_reply":"2024-12-02T16:08:16.893929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Feature Selection Impact:\n\n**Feature selection did not improve performance:**\nAccuracy (79.66%), F1-Score (59.34%), and AUC-ROC (83.78%) are slightly lower compared to the baseline Logistic Regression model trained on the full feature set.\nThis indicates that the removed features may contain valuable information for prediction.\n\n**Comparison to Baseline:**\nBaseline Accuracy: 80.01% (full feature set).\nSelected Features Accuracy: 79.66%.\n\nThe minimal performance difference suggests that feature selection is unnecessary for this dataset when using Logistic Regression.","metadata":{}},{"cell_type":"markdown","source":"### 5.2 Decision Tree Model with Hyperparameter Tuning\nThis step optimizes the Decision Tree Classifier (DT) by tuning its hyperparameters to find the best configuration for maximizing model performance. The tuned model is evaluated on the test data using standard metrics.","metadata":{}},{"cell_type":"code","source":"# Define hyperparameter grid for tuning\nparam_grid = {\n    'max_depth': [80, 60, 40, 20, 10, 5],\n    'criterion': ['gini', 'entropy', 'log_loss']\n}\n\n# Tune the Decision Tree model using cross-validation\ndt = tune_model(\n    DecisionTreeClassifier(random_state=17), \n    param_grid, \n    X_train, \n    y_train, \n    scoring='roc_auc'\n)\n\n# Compute predicted probabilities for Decision Tree\ny_test_proba_dt = dt.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = dt.predict(X_test)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_dt)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_dt, tpr_dt, _ = roc_curve(y_test, y_test_proba_dt)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:08:24.985311Z","iopub.execute_input":"2024-12-02T16:08:24.986092Z","iopub.status.idle":"2024-12-02T16:08:26.187508Z","shell.execute_reply.started":"2024-12-02T16:08:24.986053Z","shell.execute_reply":"2024-12-02T16:08:26.186405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis of Desicion Tree\n**DT Model Performance:**\n\nThe tuned Decision Tree model underperformed on the test set, with:\nAccuracy of 72.68%. \nA significant drop in F1-Score (50.11%) and AUC-ROC (65.76%).\nThese results suggest that the Decision Tree struggled to generalize and may have lost critical information due to its limited depth.\n\n**Comparison to Logistic Regression:**\n\nLogistic Regression (with or without PCA) outperformed the Decision Tree in all metrics (accuracy, F1, AUC).\nThis indicates that Logistic Regression's simplicity and linear decision boundaries may be better suited for this dataset.\n\n**Impact of Depth Restriction:**\n\nWhile restricting the depth (max_depth=5) prevents overfitting, it may have been too aggressive, reducing the model's capacity to learn meaningful patterns.\n\n**Conclusion**\n\nThe tuned Decision Tree model did not perform as well as Logistic Regression. This suggests that Decision Trees might not be the best standalone model for this dataset without further enhancements (e.g., using ensemble methods like Random Forest or Gradient Boosting).","metadata":{}},{"cell_type":"markdown","source":"### 5.3 Random Forest: Hyperparameter Tuning and Evaluation\nTo train and evaluate a Random Forest (RF) model by optimizing key hyperparameters using RandomizedSearchCV. The goal is to find the best configuration that maximizes performance on the test dataset.","metadata":{}},{"cell_type":"code","source":"# Define hyperparameter grid for tuning\nparam_grid = {\n    'criterion': ['gini', 'entropy'],         # Splitting criteria\n    'n_estimators': [50, 100, 150],          # Number of trees in the forest\n    'max_features': [40, 60, 80, 100],       # Maximum number of features to consider\n    'max_depth': [80, 50, 20, 10, 5]         # Maximum depth of the trees\n}\n\n# Tune the Random Forest model using cross-validation\nrf = tune_model(\n    RandomForestClassifier(random_state=17, n_jobs=-1), \n    param_grid, \n    X_train, \n    y_train, \n    scoring='roc_auc'\n)\n\n# Compute predicted probabilities for Random Forest\ny_test_proba_rf = rf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = rf.predict(X_test)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_rf)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_proba_rf)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:08:32.658227Z","iopub.execute_input":"2024-12-02T16:08:32.658618Z","iopub.status.idle":"2024-12-02T16:09:05.187767Z","shell.execute_reply.started":"2024-12-02T16:08:32.658587Z","shell.execute_reply":"2024-12-02T16:09:05.186577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis of Random Forest\n**Model Performance:** Random Forest achieved moderate test accuracy (79.18%) and AUC (81.93%) but did not outperform Logistic Regression. The F1-Score (57.28%) reflects limited handling of class imbalance.\n\n**Comparison to Decision Tree:** Random Forest slightly improved accuracy and AUC over the Decision Tree, demonstrating better generalization and reduced overfitting.\n\n**Depth Restriction:** The best max_depth=5 likely limited the model's ability to capture complex patterns.\n\n**Feature Subsets:** Using max_features=40 helped reduce noise but may have restricted the model's full potential.\n\n**Conclusion:**\nRandom Forest showed slight improvements over the Decision Tree but fell short of Logistic Regression. Increasing max_depth and max_features could improve performance.","metadata":{}},{"cell_type":"markdown","source":"### 5.4 Support Vector Machine (SVM): Hyperparameter Tuning and Evaluation\nTo optimize and evaluate an SVM classifier by tuning hyperparameters such as C, gamma, and kernel using cross-validation. The goal is to achieve high performance on the test dataset, focusing on the AUC metric.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n\n# Define hyperparameter grid for tuning\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\nparam_grid = [\n    {'C': param_range, 'kernel': ['linear']},\n    {'C': param_range, 'gamma': param_range, 'kernel': ['rbf']}\n]\n\n# SVC model initialization with `probability=True`\nsvm = tune_model(\n    SVC(probability=True, random_state=17), \n    param_grid, \n    X_train, \n    y_train, \n    scoring='roc_auc'\n)\n\n# Compute predicted probabilities for SVM\ny_test_proba_svm = svm.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = svm.predict(X_test)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_svm)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_svm, tpr_svm, _ = roc_curve(y_test, y_test_proba_svm)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:09:11.781862Z","iopub.execute_input":"2024-12-02T16:09:11.782262Z","iopub.status.idle":"2024-12-02T16:11:15.040246Z","shell.execute_reply.started":"2024-12-02T16:09:11.782230Z","shell.execute_reply":"2024-12-02T16:11:15.039160Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis of SVM Model\n**Model Performance:**\n\nThe SVM model achieved moderate accuracy (79.60%) and AUC (78.71%) but struggled with balancing precision and recall, as reflected by the F1-Score (55.02%).\nThis indicates that the SVM had difficulty handling the class imbalance.\n\n**Hyperparameter Impact:**\n\nThe use of the RBF kernel suggests that a non-linear decision boundary was more effective than a linear one.\nA low C value (0.1) implies that stronger regularization was beneficial, likely preventing overfitting.\n\n**Comparison to Other Models:**\n\nThe SVM's performance was similar to Random Forest and Decision Tree but did not outperform Logistic Regression in any metric.\n\n**Conclusion:**\nSVM with the RBF kernel provided reasonable results but did not outperform simpler models like Logistic Regression. Its sensitivity to hyperparameters and the need for scaling make it less favorable for this dataset.","metadata":{}},{"cell_type":"markdown","source":"### 5.5 K-Nearest Neighbors (KNN): Hyperparameter Tuning and Evaluation\nTo optimize the K-Nearest Neighbors (KNN) model by tuning the n_neighbors hyperparameter to identify the optimal number of neighbors. The goal is to achieve the best performance in terms of AUC and other metrics on the test dataset.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output\n\n# Define hyperparameter grid for tuning\nparam_grid = {'n_neighbors': [4, 6, 8, 10, 12]}\n\n# Tune the KNN model using cross-validation\nknn = tune_model(\n    KNeighborsClassifier(), \n    param_grid, \n    X_train, \n    y_train, \n    scoring='roc_auc'\n)\n\n# Compute predicted probabilities for KNN\ny_test_proba_knn = knn.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = knn.predict(X_test)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_knn)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_knn, tpr_knn, _ = roc_curve(y_test, y_test_proba_knn)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:11:26.770683Z","iopub.execute_input":"2024-12-02T16:11:26.771075Z","iopub.status.idle":"2024-12-02T16:11:27.452030Z","shell.execute_reply.started":"2024-12-02T16:11:26.771040Z","shell.execute_reply":"2024-12-02T16:11:27.450917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis of KNN\n**Model Performance:**\n\nThe KNN model achieved moderate test accuracy (75.52%) and AUC (76.13%), but the F1-Score (53.48%) highlights challenges in handling class imbalance.\n\n**Hyperparameter Impact:**\n\nThe optimal n_neighbors=12 suggests that considering a larger number of neighbors helped smooth out predictions but may have diluted the impact of closer neighbors, limiting the model's ability to capture fine-grained patterns.\n\n**Comparison to Other Models:**\n\nKNNâ€™s accuracy and AUC are lower than Logistic Regression, Random Forest, and SVM, indicating it is less effective for this dataset.\nIts sensitivity to feature scaling and distance-based calculations may contribute to its lower performance.\n\n**Conclusion:**\nThe KNN model provided lower performance compared to other models, with limited ability to balance class predictions effectively. Its reliance on the choice of neighbors and sensitivity to scaling makes it less suitable for this dataset.","metadata":{}},{"cell_type":"markdown","source":"### 5.6 Voting Classifier: Model Combination and Evaluation\nTo combine multiple individual classifiers into a Voting Classifier and evaluate its performance. The aim is to leverage the strengths of each model by combining their predictions in a \"soft\" voting mechanism to improve overall accuracy and robustness.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n\n# Define individual classifiers\nclf1 = LogisticRegression(solver='liblinear', C=10.0, random_state=17)             # Logistic Regression\nclf2 = DecisionTreeClassifier(max_depth=5, criterion='entropy', random_state=17)   # Decision Tree\nclf3 = SVC(probability=True, kernel='rbf', gamma=0.01, C=0.1, random_state=17)     # Support Vector Machine\nclf4 = KNeighborsClassifier(n_neighbors=4)                                         # K-Nearest Neighbors\n\n# Define the Voting Classifier with \"soft\" voting\nvoting_clf = VotingClassifier(\n    estimators=[('lr', clf1), ('dt', clf2), ('svm', clf3), ('knn', clf4)],\n    voting='soft'\n)\n\n# Train the Voting Classifier on the training data\nvoting_clf.fit(X_train, y_train)\n\n# Compute predicted probabilities for the test set\ny_test_proba_voting_clf = voting_clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_pred = voting_clf.predict(X_test)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_pred)  # Accuracy\nf1 = f1_score(y_test, y_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_voting_clf)\n\n# Print evaluation metrics\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_voting_clf, tpr_voting_clf, _ = roc_curve(y_test, y_test_proba_voting_clf)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:11:35.007027Z","iopub.execute_input":"2024-12-02T16:11:35.007404Z","iopub.status.idle":"2024-12-02T16:11:37.786684Z","shell.execute_reply.started":"2024-12-02T16:11:35.007370Z","shell.execute_reply":"2024-12-02T16:11:37.783903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis of Voting Classifier\n**Model Combination:**\n\nThe Voting Classifier combines predictions from Logistic Regression, Decision Tree, SVM, and KNN using \"soft\" voting, which averages probabilities from each model to make predictions.\nThe ensemble approach helps leverage the strengths of each model but does not significantly outperform the best individual model (e.g., Logistic Regression).\n\n**Performance Comparison:**\nThe Voting Classifierâ€™s accuracy (79.54%) and AUC (82.84%) are similar to those of Logistic Regression, indicating limited added value from the ensemble.\n\n**Class Imbalance:**\nThe moderate F1-Score (59.00%) suggests that the Voting Classifier struggles with class imbalance, similar to individual models.\n\n**Conclusion:**\nThe Voting Classifier provided comparable performance to the best individual models but did not significantly improve overall metrics. This suggests limited synergy between the chosen classifiers.","metadata":{}},{"cell_type":"markdown","source":"### 5.7 Bagging Classifier: Implementation and Evaluation\nTo evaluate the performance of a Bagging Classifier, an ensemble method that trains multiple instances of a base classifier (in this case, a Decision Tree) on different subsets of the data. The goal is to improve model stability and reduce variance compared to a single Decision Tree.","metadata":{}},{"cell_type":"code","source":"# Define the base estimator: Decision Tree\ntree = DecisionTreeClassifier(\n    criterion='entropy', \n    max_depth=None, \n    random_state=17\n)\n\n# Define the Bagging Classifier\nbag = BaggingClassifier(\n    estimator=tree,               # Base classifier\n    n_estimators=500,             # Number of base classifiers\n    max_samples=1.0,              # Use all samples (with replacement)\n    max_features=0.4,             # Use 40% of features for each base estimator\n    bootstrap=True,               # Bootstrap samples\n    bootstrap_features=False,     # Do not bootstrap features\n    n_jobs=-1,                    # Use all processors for parallel computation\n    random_state=17\n)\n\n# Train the Bagging Classifier\nbag = bag.fit(X_train, y_train)\n\n# Compute predicted probabilities for the Bagging Classifier\ny_test_proba_bag = bag.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_test_pred = bag.predict(X_test)\n\n# Evaluate the model using hard predictions\naccuracy = accuracy_score(y_test, y_test_pred)  # Accuracy\nf1 = f1_score(y_test, y_test_pred)              # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_bag)\n\n# Print evaluation metrics\nprint(f'Bagging Train Accuracy: {accuracy_score(y_train, bag.predict(X_train)):.3f}')\nprint(f'Bagging Test Accuracy: {accuracy:.3f}')\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_bag, tpr_bag, _ = roc_curve(y_test, y_test_proba_bag)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:11:44.688495Z","iopub.execute_input":"2024-12-02T16:11:44.688889Z","iopub.status.idle":"2024-12-02T16:11:49.019956Z","shell.execute_reply.started":"2024-12-02T16:11:44.688855Z","shell.execute_reply":"2024-12-02T16:11:49.018696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis of Bagging Classifier\n**Training Accuracy (99.0%):** Indicates overfitting due to unpruned Decision Trees (max_depth=None), which memorize the training data.\n\n**Test Accuracy (79.0%):** Suggests moderate generalization but highlights a significant drop from training accuracy.\n\n**ROC-AUC (82.27%):** Demonstrates good ranking ability, outperforming test accuracy, and indicating robust performance in distinguishing classes.\n\n**Conclusion:**\nThe Bagging Classifier performs reasonably well on the test data with a strong ROC-AUC but suffers from overfitting, as shown by the training-test accuracy gap. Further tuning is recommended.","metadata":{}},{"cell_type":"markdown","source":"### 5.8 XGBoost Classifier: Implementation and Evaluation\nTo evaluate the performance of the XGBoost Classifier, a powerful gradient boosting algorithm, by training it on the dataset and calculating relevant metrics, including accuracy and ROC-AUC.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\nimport xgboost as xgb\n\n# Initialize the XGBoost Classifier\nxgb_clf = xgb.XGBClassifier(\n    n_estimators=500,        # Number of boosting rounds\n    learning_rate=0.01,      # Step size shrinkage to prevent overfitting\n    max_depth=4,             # Maximum tree depth for base learners\n    random_state=17          # Ensure reproducibility\n)\n\n# Train the XGBoost Classifier\nxgb_clf.fit(X_train, y_train)\n\n# Compute predicted probabilities for XGBoost\ny_test_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n\n# Make hard predictions on the test set\ny_test_pred = xgb_clf.predict(X_test)\n\n# Evaluate the model using hard predictions\nxgb_train_accuracy = accuracy_score(y_train, xgb_clf.predict(X_train))  # Training Accuracy\nxgb_test_accuracy = accuracy_score(y_test, y_test_pred)                 # Test Accuracy\nf1 = f1_score(y_test, y_test_pred)                                      # F1 Score\n\n# Evaluate the ROC-AUC using probabilities\nroc_auc = roc_auc_score(y_test, y_test_proba_xgb)\n\n# Print evaluation metrics\nprint(f'XGBoost Train Accuracy: {xgb_train_accuracy:.3f}')\nprint(f'XGBoost Test Accuracy: {xgb_test_accuracy:.3f}')\nprint(f\"Test F1: {f1:.4f}\")\nprint(f\"Test ROC-AUC: {roc_auc:.4f}\")\n\n# Prepare values for later plotting (store in variables or dictionary)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_test_proba_xgb)  # For ROC curve plotting later\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:11:53.815528Z","iopub.execute_input":"2024-12-02T16:11:53.815980Z","iopub.status.idle":"2024-12-02T16:11:54.284154Z","shell.execute_reply.started":"2024-12-02T16:11:53.815940Z","shell.execute_reply":"2024-12-02T16:11:54.283235Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Analysis\n**Model Performance:** Training accuracy (82.8%) and test accuracy (79.2%) are close, indicating minimal overfitting and good generalization. The ROC-AUC (83.74%) shows strong class distinction.\n\n**Strengths of XGBoost:** Outperforms standalone models (e.g., Decision Tree, KNN) by reducing bias and variance. Gradual boosting with a learning rate of 0.01 and 500 estimators enhances stability.\n\n**Comparison:** Achieves the highest ROC-AUC among tested models, highlighting its ability to rank predictions effectively.\n\n**Conclusion:**\nXGBoost delivers strong performance, balancing generalization and ranking ability better than previous models.","metadata":{}},{"cell_type":"markdown","source":"### 5.9 XGBoost Classifier: Hyperparameter Tuning and Evaluation\nTo improve the XGBoost Classifier's performance by using RandomizedSearchCV to find the best hyperparameters and evaluate its performance on the test set. Metrics include accuracy, F1 score, ROC-AUC, and an optimized F1 score based on threshold adjustment.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, roc_auc_score, accuracy_score, f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\n\n# Define updated parameter grid\nparam_grid = {\n    'n_estimators': [300, 500, 700],        # Number of boosting rounds\n    'learning_rate': [0.01, 0.02, 0.05],   # Learning rate for fine updates\n    'max_depth': [4, 5, 6],                # Allow more complex trees\n    'min_child_weight': [1, 2],            # Looser constraints on leaf nodes\n    'subsample': [0.8, 0.9, 1.0],          # Higher data usage\n    'colsample_bytree': [0.8, 0.9, 1.0],   # Higher feature usage\n    'gamma': [0, 0.1, 0.2],                # Regularization for tree splits\n    'reg_alpha': [0.0, 0.1],               # Reduced L1 regularization\n    'reg_lambda': [0.5, 1.0, 1.5],         # Reduced L2 regularization\n}\n\n# Initialize the XGBoost classifier\nxgb_clf = xgb.XGBClassifier(\n    random_state=17,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\n# Set up RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    estimator=xgb_clf,\n    param_distributions=param_grid,\n    n_iter=50,              # Number of settings to sample\n    scoring='roc_auc',      # Optimize for ROC-AUC\n    cv=5,                   # Stratified k-fold cross-validation\n    verbose=0,\n    random_state=42,\n    n_jobs=-1               # Use all CPU cores\n)\n\n# Fit the model with RandomizedSearchCV\nrandom_search.fit(X_train, y_train)\n\n# Get the best model and parameters\nbest_xgb = random_search.best_estimator_\nprint(f\"Best Parameters: {random_search.best_params_}\")\n\n# Evaluate on the test set\ny_test_proba_best_xgb = best_xgb.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\ny_test_pred = best_xgb.predict(X_test)\n\n# Performance metrics\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_f1 = f1_score(y_test, y_test_pred)\ntest_auc = roc_auc_score(y_test, y_test_proba_best_xgb)\n\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test F1 Score: {test_f1:.4f}\")\nprint(f\"Test ROC-AUC: {test_auc:.4f}\")\n\n# Threshold Optimization\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_test_proba_best_xgb)\nf1_scores = 2 * (precisions * recalls) / (precisions + recalls)\noptimal_threshold_idx = f1_scores.argmax()\noptimal_threshold = thresholds[optimal_threshold_idx]  # Get the optimal threshold\nprint(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n\n# Reevaluate with the optimal threshold\ny_test_pred_opt = (y_test_proba_best_xgb >= optimal_threshold).astype(int)\ntest_f1_opt = f1_score(y_test, y_test_pred_opt)\nprint(f\"Optimized Test F1 Score: {test_f1_opt:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:12:16.567074Z","iopub.execute_input":"2024-12-02T16:12:16.567438Z","iopub.status.idle":"2024-12-02T16:13:02.246498Z","shell.execute_reply.started":"2024-12-02T16:12:16.567408Z","shell.execute_reply":"2024-12-02T16:13:02.245706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Improvements:\n**Threshold Optimization:** F1 score increased from 0.5473 to 0.6389, balancing precision and recall effectively.\n\n**ROC-AUC Stability:** Both models maintained strong ROC-AUC scores (0.8374 and 0.8372), ensuring reliability.\n\nThe tuned model includes threshold optimization, which improves its F1 score to 0.6389. This gives it a clear advantage when precision-recall balance is critical.","metadata":{}},{"cell_type":"markdown","source":"## 6. Code Implementation Comparison\n### 6.1 Performance Metrics Table","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Define the metrics dictionary\nmodel_metrics = {\n    \"Logistic Regression (Baseline)\": {\"Accuracy\": 0.8001, \"F1-Score\": 0.6024, \"ROC-AUC\": 0.8383},\n    \"Logistic Regression (Feature Selection)\": {\"Accuracy\": 0.7966, \"F1-Score\": 0.5934, \"ROC-AUC\": 0.8378},\n    \"Decision Tree\": {\"Accuracy\": 0.7268, \"F1-Score\": 0.5011, \"ROC-AUC\": 0.6576},\n    \"Random Forest\": {\"Accuracy\": 0.7918, \"F1-Score\": 0.5728, \"ROC-AUC\": 0.8193},\n    \"SVM\": {\"Accuracy\": 0.7960, \"F1-Score\": 0.5502, \"ROC-AUC\": 0.7871},\n    \"KNN\": {\"Accuracy\": 0.7552, \"F1-Score\": 0.5348, \"ROC-AUC\": 0.7613},\n    \"Voting Classifier\": {\"Accuracy\": 0.7954, \"F1-Score\": 0.5884, \"ROC-AUC\": 0.8284},\n    \"Bagging Classifier\": {\"Accuracy\": 0.7900, \"F1-Score\": 0.5298, \"ROC-AUC\": 0.8227},\n    \"XGBoost (Tunned) \": {\"Accuracy\": 0.7847, \"F1-Score\": 0.6389, \"ROC-AUC\": 0.8372},\n}\n\n# Convert metrics dictionary to a DataFrame\nmetrics_df = pd.DataFrame(model_metrics).T  # Transpose to make models the rows\nmetrics_df.index.name = \"Model\"  # Add a name to the index\nmetrics_df.reset_index(inplace=True)  # Move the index to a regular column\n\n# Round metrics to 4 decimal places\nmetrics_df = metrics_df.round(4)\n\n# Print the table with a better format\nprint(metrics_df.to_markdown(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:13:15.744816Z","iopub.execute_input":"2024-12-02T16:13:15.745273Z","iopub.status.idle":"2024-12-02T16:13:15.792989Z","shell.execute_reply.started":"2024-12-02T16:13:15.745238Z","shell.execute_reply":"2024-12-02T16:13:15.791897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.2 Visualize Metrics\nGrouped Bar Chart for Accuracy, F1-Score, and ROC-AUC","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Convert metrics to DataFrame for plotting\nmetrics_df = pd.DataFrame(model_metrics).T  # Transpose to have models as rows\nmetrics_df.reset_index(inplace=True)        # Reset index for plotting\nmetrics_df.rename(columns={\"index\": \"Model\"}, inplace=True)\n\n# Plot grouped bar chart\nmetrics_df.plot(x=\"Model\", kind=\"bar\", figsize=(14, 8), legend=True)\nplt.title(\"Comparison of Model Metrics\")\nplt.ylabel(\"Scores\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Metrics\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:15:15.702073Z","iopub.execute_input":"2024-12-02T16:15:15.703066Z","iopub.status.idle":"2024-12-02T16:15:16.221921Z","shell.execute_reply.started":"2024-12-02T16:15:15.703019Z","shell.execute_reply":"2024-12-02T16:15:16.220817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.3 Plotting the ROC Curves\nThis will compare the ranking ability of all models. The ROC (Receiver Operating Characteristic) curve compares the True Positive Rate (TPR) against the False Positive Rate (FPR) for different classification thresholds. The Area Under the Curve (AUC) quantifies the model's ability to distinguish between positive and negative classes.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Ensure model_metrics is populated correctly\nmodel_metrics = {\n    \"Logistic Regression (Baseline)\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_lr_baseline)},\n    \"Decision Tree\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_dt)},\n    \"Random Forest\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_rf)},\n    \"KNN\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_knn)},\n    \"SVM\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_svm)},\n    \"Voting Classifier\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_voting_clf)},\n    \"Bagging Classifier\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_bag)},\n    \"XGBoost\": {\"ROC-AUC\": roc_auc_score(y_test, y_test_proba_xgb)},\n}\n\n# Plotting ROC Curves\nplt.figure(figsize=(10, 8))\n\n# Define model probabilities and labels for plotting\nmodels = {\n    \"Logistic Regression (Baseline)\": y_test_proba_lr_baseline,\n    \"Decision Tree\": y_test_proba_dt,\n    \"Random Forest\": y_test_proba_rf,\n    \"KNN\": y_test_proba_knn,\n    \"SVM\": y_test_proba_svm,\n    \"Voting Classifier\": y_test_proba_voting_clf,\n    \"Bagging Classifier\": y_test_proba_bag,\n    \"XGBoost\": y_test_proba_xgb,\n}\n\n# Iterate over models and plot their ROC curves\nfor model_name, y_proba in models.items():\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    auc_score = model_metrics[model_name][\"ROC-AUC\"]\n    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {auc_score:.2f})\")\n\n# Plot configuration\nplt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Chance\")\nplt.title(\"ROC Curves for All Models\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:15:22.336506Z","iopub.execute_input":"2024-12-02T16:15:22.336923Z","iopub.status.idle":"2024-12-02T16:15:22.856106Z","shell.execute_reply.started":"2024-12-02T16:15:22.336885Z","shell.execute_reply":"2024-12-02T16:15:22.855042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ROC Curve Results:\n\n**XGBoost (AUC = 0.84):**\n\nXGBoost demonstrates the best performance among all models, achieving the highest AUC. Its curve closely follows the top-left corner, signifying strong discriminatory power and an excellent balance between sensitivity (True Positive Rate) and specificity (False Positive Rate). XGBoost remains the most robust candidate for the final model due to its high AUC and superior ability to handle complex patterns in the data.\n\n**Logistic Regression (AUC = 0.84)**\n\nLogistic Regression shows excellent performance, matching XGBoost in terms of AUC. This is the second-best performer. Its simplicity and interpretability make it an attractive option. Despite being a baseline model, its performance demonstrates that linear models can be highly effective in certain contexts. \n\n**Voting Classifier (AUC = 0.83):**\n\nThe Voting Classifier also performs very well. The performance is slightly below XGBoost and LR but still competitive.\nThis suggests that combining multiple models (Logistic Regression, Decision Tree, SVM, KNN) has synergistic benefits, improving the ability to rank predictions.\n\n**Bagging Classifier (AUC = 0.82):**\n\nBagging provides strong performance, with an AUC comparable to the Voting Classifier. Its curve is smooth and indicates robust classification across thresholds. However, it falls slightly behind XGBoost, Logistic Regression and the Voting Classifier in ranking ability. Bagging remains a solid choice if simplicity and robustness are prioritized.\n\n**Random Forest (AUC = 0.82)** \n\nRandom Forest matches the AUC of Bagging, showcasing its ability to rank predictions effectively. However, it does not outperform XGBoost or the Voting Classifier, indicating that boosting methods and other ensemble strategies might handle the dataset complexity better.\n\n**KNN (AUC = 0.76) and SVM (AUC = 0.79)**\n\nBoth KNN and SVM show moderate performance but underperform compared to ensemble methods and Logistic Regression. Their curves indicate less effective handling of class imbalance, with flatter ROC curves suggesting weaker ranking ability.\n\n**Decision Tree (AUC = 0.66)**\n\nThe Decision Tree is the weakest model, with the lowest AUC and the most distant curve from the top-left corner. Its performance indicates significant limitations in handling the dataset's complexity and distinguishing between classes effectively.\n\n**Conclusion:**\n\nBoth **XGBoost** and **Logistic Regression** are the strongest candidates for the final model, as they achieve the same **AUC (0.84)** and very similar scores.\n\nThe **Voting Classifier** is a strong alternative if an ensemble approach is desired, while **Bagging** and **Random Forest** provide solid, but not top-tier, performance. Individual classifiers like **KNN, SVM,** and **Decision Tree** are less effective in this task.","metadata":{}},{"cell_type":"markdown","source":"## 7. Submission\n### 7.1 Logistic Regression (Baseline) - score 85.625%","metadata":{}},{"cell_type":"code","source":"# Generate submission file using the LR model\nmodel=lr_baseline\nwrite_predictions(model, test_input_path='/kaggle/input/customer-churn-prediction-fall-2024/test.csv', \n                      predict_out_path='lr_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:15:40.164973Z","iopub.execute_input":"2024-12-02T16:15:40.165378Z","iopub.status.idle":"2024-12-02T16:15:40.229448Z","shell.execute_reply.started":"2024-12-02T16:15:40.165345Z","shell.execute_reply":"2024-12-02T16:15:40.227985Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7.2 XBoost Tunned - score 85.634%","metadata":{}},{"cell_type":"code","source":"# Generate submission file\nwrite_predictions(\n    model=best_xgb, \n    test_input_path='/kaggle/input/customer-churn-prediction-fall-2024/test.csv', \n    predict_out_path='best_xgb_submission.csv'  # Updated file name for clarity\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T16:15:43.385775Z","iopub.execute_input":"2024-12-02T16:15:43.386213Z","iopub.status.idle":"2024-12-02T16:15:43.433155Z","shell.execute_reply.started":"2024-12-02T16:15:43.386180Z","shell.execute_reply":"2024-12-02T16:15:43.430567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Conclusion:\n\nWhile **Logistic Regression** performed exceptionally well, achieving an score of **85.625%**, we ultimately finalized **XGBoost**, which achieved a slightly better score of **85.634%** after tuning. This marginal improvement might seem small, but there are three key reasons for our decision:\n\n**1) Threshold Optimization:** We tuned XGBoost with an optimal threshold, improving its F1-Score to 0.6389 compared to Logistic Regression's 0.6024. This makes XGBoost superior when balancing precision and recall is criticalâ€”an essential factor in churn prediction.\n\n**2. Flexibility and Potential:** Feature selection and PCA did not improve Logistic Regression, suggesting limited room for optimization. In contrast, XGBoost provided opportunities for fine-tuning, demonstrating its ability to adapt to the dataset.\n\n**3. Probability-Based Decisions:** Both models provide probabilities, but XGBoostâ€™s enhanced ranking ability and flexibility ensure itâ€™s better equipped for prioritizing customers with high churn probabilities.\n\nAdditionally, **Voting Classifier, Bagging, and Random Forest** all achieved similar **AUC scores (0.82â€“0.83)**, demonstrating good performance and effective ranking abilities. However, they slightly lagged behind XGBoost and Logistic Regression in handling the dataset's complexity and optimizing precision-recall trade-offs.\n\n**In conclusion,** while Logistic Regression offered simplicity and competitive performance, **XGBoost emerged as the stronger candidate** for predicting churn. Its threshold optimization, improved F1-Score, and robust ranking ability give it a clear edge. By leveraging the predicted probabilities, XGBoost empowers the company to take precise, data-driven actions to retain high-risk customers.","metadata":{}}]}